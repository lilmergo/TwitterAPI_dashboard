{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b45cc5-4bec-42c9-8d8b-be952b58d233",
   "metadata": {},
   "source": [
    "# Imports, environmental variables, and API authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07e281d5-9ae5-4cdf-8179-dcb582cd3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "032ff01f-f2f7-4dc6-8363-c8cf900625ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\megan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#environment variables\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAANqgWAEAAAAAjebgE2riBRW%2BRY3bLEfWyRXSEao%3DuZnQFTlyRppafAa24BQiDDf0TIYaYgpSpTgDMu5anfZ8zPNelr\"\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "#download sentiment analyzer script\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9765c-6e55-4499-bf8b-f7686a48ac2b",
   "metadata": {},
   "source": [
    "# Build a query and specify number of API calls\n",
    "In the string variable 'topic', specify query parameters to search for the tweet you want. It can be as simple as a single keyword or a hashtag. OR operators and brackets work, if no operators are specified between keywords an AND operator is implied. The sample query is 'KelloggsStrike OR KelloggStrike OR BoycottKelloggs OR (Kellogg (strike OR striking OR boycott OR scab))'.\n",
    "\n",
    "For more details on how to build a more complex query, check https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "\n",
    "Then specify how many API calls you would like to make in variable 'n' (up to 450). Since making 450 calls would take a while, for testing a smaller number is recommended.\n",
    "\n",
    "This will print n lines of '200' to signal that the API call was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56df3b58-f3ee-46e2-bcd5-f48a61270d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#replace sample topic with your own query to test\n",
    "topic = 'dune'\n",
    "#specify number of API calls to make\n",
    "n=10\n",
    "\n",
    "query_params = {\n",
    "    'query': topic+' lang:en',\n",
    "    'tweet.fields': 'public_metrics,entities,created_at,lang,geo,referenced_tweets',\n",
    "    'user.fields':'public_metrics,verified',\n",
    "    'expansions':'author_id',\n",
    "    'max_results':'100'\n",
    "}\n",
    "    \n",
    "#Connecting to api and running query and getting json response\n",
    "json_response = connect_to_endpoint(search_url, query_params)\n",
    "    \n",
    "#Pagination:get next page if available based on next_token collected from the first call\n",
    "next_token=json_response.get('meta').get('next_token')\n",
    "count = 0\n",
    "while count<(n-1) and next_token!=None:\n",
    "    count+=1\n",
    "    query_params = {\n",
    "    'query': topic+' lang:en',\n",
    "    'tweet.fields': 'public_metrics,entities,created_at,lang,geo,referenced_tweets',\n",
    "    'user.fields':'public_metrics,verified',\n",
    "    'expansions':'author_id',\n",
    "    'max_results':'100',\n",
    "    'next_token':next_token    \n",
    "    }\n",
    "    new_data = connect_to_endpoint(search_url, query_params)\n",
    "    \n",
    "    #append data to the original json response\n",
    "    for item in new_data['data']:\n",
    "        json_response['data'].append(item)\n",
    "    for item in new_data['includes']['users']:\n",
    "        if item not in json_response['includes']['users']:#only add unique users\n",
    "            json_response['includes']['users'].append(item) \n",
    "    next_token=new_data.get('meta').get('next_token')\n",
    "\n",
    "#write to json file\n",
    "json_response.pop('meta') \n",
    "with open(os.getcwd()+'\\\\json_responses\\\\'+'KelloggStrike.json', 'w') as f:\n",
    "    json.dump(json_response, f, sort_keys = True, indent=4)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e09da3-6009-4f64-a782-50b089e190cd",
   "metadata": {},
   "source": [
    "# Convert the JSON file to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb1d18da-4894-41c5-8544-327459308aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonToCsv(jsonData, case, folder):\n",
    "    #write to Tweet.csv\n",
    "    if case=='tweet':\n",
    "        data_file = open(os.getcwd()+'\\\\'+folder+'\\\\'+'Tweet.csv', 'w',encoding='utf-8',newline='')\n",
    "        csv_writer = csv.writer(data_file)\n",
    "        \n",
    "        #referenced tweets\n",
    "        data_file_RT = open(os.getcwd()+'\\\\'+folder+'\\\\'+'Referenced_Tweet.csv', 'w',encoding='utf-8',newline='')\n",
    "        csv_writer_RT = csv.writer(data_file_RT)\n",
    "        count = 0\n",
    "        for var in jsonData:\n",
    "            if count == 0:\n",
    "                # Writing headers of CSV file\n",
    "                header = ['tweet_id','user_id','created_at','text','like_count','quote_count','reply_count','retweet_count','sentiment_score']\n",
    "                csv_writer.writerow(header)\n",
    "                \n",
    "                header = ['tweet_id','referenced_tweet_id','type']\n",
    "                csv_writer_RT.writerow(header)\n",
    "                count += 1\n",
    "            # Writing data of CSV file\n",
    "            \n",
    "            #calculate sentiment\n",
    "            score=sia.polarity_scores(var.get('text'))[\"compound\"]\n",
    "            \n",
    "            metrics=var.get('public_metrics')\n",
    "            row=[var.get('id'),var.get('author_id'),var.get('created_at'),\"'\"+var.get('text')+\"'\",metrics['like_count'],metrics['quote_count'],metrics['reply_count'],metrics['retweet_count'], score]\n",
    "            csv_writer.writerow(row)\n",
    "            \n",
    "            #rt\n",
    "            if 'referenced_tweets' in var:\n",
    "                rts=var.get('referenced_tweets')\n",
    "                for rt in rts:\n",
    "                    row=[var.get('id'),rt['id'], rt['type']]\n",
    "                    csv_writer_RT.writerow(row)\n",
    "        data_file.close()\n",
    "        data_file_RT.close()\n",
    "    #write to User.csv\n",
    "    if case=='user':\n",
    "        data_file = open(os.getcwd()+'\\\\'+folder+'\\\\'+'User.csv', 'w',encoding='utf-8', newline='')\n",
    "        csv_writer = csv.writer(data_file)\n",
    "        count = 0\n",
    "        for var in jsonData:\n",
    "            if count == 0:\n",
    "                # Writing headers of CSV file\n",
    "                header = ['user_id','followers','following','tweet_count','listed_count','verified']\n",
    "                csv_writer.writerow(header)\n",
    "                count += 1\n",
    "            # Writing data of CSV file\n",
    "            metrics=var.get('public_metrics')\n",
    "            row=[var.get('id'),metrics['followers_count'],metrics['following_count'],metrics['tweet_count'],metrics['listed_count'], var.get('verified') ]\n",
    "            csv_writer.writerow(row)\n",
    "        data_file.close()\n",
    "    ##write to Hashtag_Tweet.csv, URL_Tweet.csv, Mention_Tweet.csv\n",
    "    if case =='entities':\n",
    "        data_file_HT = open(os.getcwd()+'\\\\'+folder+'\\\\'+'Hashtag_Tweet.csv', 'w',encoding='utf-8', newline='')\n",
    "        csv_writer_HT = csv.writer(data_file_HT)\n",
    "        \n",
    "        data_file_UT = open(os.getcwd()+'\\\\'+folder+'\\\\'+'URL_Tweet.csv', 'w',encoding='utf-8', newline='')\n",
    "        csv_writer_UT = csv.writer(data_file_UT)\n",
    "            \n",
    "        data_file_MT = open(os.getcwd()+'\\\\'+folder+'\\\\'+'Mention_Tweet.csv', 'w',encoding='utf-8', newline='')\n",
    "        csv_writer_MT = csv.writer(data_file_MT)\n",
    "            \n",
    "        count = 0\n",
    "        for var in jsonData:\n",
    "            if count == 0:\n",
    "                    # Writing headers of CSV file\n",
    "                header = ['tweet_id','hashtag']\n",
    "                csv_writer_HT.writerow(header)\n",
    "                    \n",
    "                header = ['tweet_id','urls']\n",
    "                csv_writer_UT.writerow(header)\n",
    "                    \n",
    "                header = ['tweet_id','mentions']\n",
    "                csv_writer_MT.writerow(header)\n",
    "                count += 1\n",
    "                    \n",
    "                # Writing data of CSV file\n",
    "            if 'entities' in var:\n",
    "                entities=var.get('entities')\n",
    "                if 'hashtags' in entities:\n",
    "                    for hashtag in entities['hashtags']:\n",
    "                        row=[var.get('id'),hashtag.get('tag')]\n",
    "                        csv_writer_HT.writerow(row)\n",
    "                if 'urls' in entities:\n",
    "                    for url in entities['urls']:\n",
    "                        row=[var.get('id'),url.get('expanded_url')]\n",
    "                        csv_writer_UT.writerow(row)\n",
    "                if 'mentions' in entities:\n",
    "                    for mention in entities['mentions']:\n",
    "                        row=[var.get('id'),mention.get('id')]\n",
    "                        csv_writer_MT.writerow(row)\n",
    "        data_file_HT.close()\n",
    "        data_file_UT.close()\n",
    "        data_file_MT.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61770f6-b531-442f-b0ec-31fb9f7996aa",
   "metadata": {},
   "source": [
    "## For testing, specify the new test folder name in the variable 'folderName'\n",
    "A new test folder will be created containing the newly generated CSV files. If the folder already exists then overwrite the CSV files in the specified folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fc39005-39fd-4976-ba80-40ec81590027",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd()+'\\\\json_responses\\\\'+'KelloggStrike.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "jsonData=data['data']\n",
    "jsonUser=data['includes']['users']\n",
    "\n",
    "#specify folder name here\n",
    "folderName = 'Test4'\n",
    "#Generate CSV files into the new folder, if folder already exists, overwrite CSV files in that folder\n",
    "location=os.getcwd()+'\\\\'+folderName\n",
    "if (path.exists(location)==False):\n",
    "    os.mkdir(location)\n",
    "jsonToCsv(jsonData, 'tweet', folderName)\n",
    "jsonToCsv(jsonData, 'entities',folderName)\n",
    "jsonToCsv(jsonUser, 'user',folderName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c22685-e083-4c01-9b39-d9f2eb0cef6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
