{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eb049e4-2f3a-483f-bcaf-67c791620d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "032ff01f-f2f7-4dc6-8363-c8cf900625ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\megan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#environment variables\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAANqgWAEAAAAAjebgE2riBRW%2BRY3bLEfWyRXSEao%3DuZnQFTlyRppafAa24BQiDDf0TIYaYgpSpTgDMu5anfZ8zPNelr\"\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "#script\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2188d4-6c3f-470f-87ff-178915915182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for authentication and conntecting to api endpoints\n",
    "def bearer_oauth(r):\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56df3b58-f3ee-46e2-bcd5-f48a61270d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#all topics\n",
    "topics = ['KelloggsStrike OR KelloggStrike OR BoycottKelloggs OR (Kellogg (strike OR striking OR boycott OR scab))']\n",
    "#turn json to string and write to json file\n",
    "\n",
    "for topic in topics:\n",
    "    #for topic in topics:\n",
    "    #replace query parameter \n",
    "    #multiple queries to limit nesting in JSON\n",
    "    query_params = {\n",
    "        'query': topic+' lang:en',\n",
    "        'tweet.fields': 'public_metrics,entities,created_at,lang,geo,referenced_tweets',\n",
    "        'user.fields':'public_metrics,verified',\n",
    "        'expansions':'author_id',\n",
    "        'max_results':'100'\n",
    "    }\n",
    "    \n",
    "    #getting json\n",
    "    json_response = connect_to_endpoint(search_url, query_params)\n",
    "    \n",
    "    #get next page if available\n",
    "    next_token=json_response.get('meta').get('next_token')\n",
    "    count = 0\n",
    "    while count<449 and next_token!=None:\n",
    "        count+=1\n",
    "        query_params = {\n",
    "        'query': topic+' lang:en',\n",
    "        'tweet.fields': 'public_metrics,entities,created_at,lang,geo,referenced_tweets',\n",
    "        'user.fields':'public_metrics,verified',\n",
    "        'expansions':'author_id',\n",
    "        'max_results':'100',\n",
    "        'next_token':next_token    \n",
    "        }\n",
    "        new_data = connect_to_endpoint(search_url, query_params)\n",
    "        for item in new_data['data']:\n",
    "            json_response['data'].append(item)\n",
    "        for item in new_data['includes']['users']:\n",
    "            if item not in json_response['includes']['users']:#only add unique users\n",
    "                json_response['includes']['users'].append(item) \n",
    "        next_token=new_data.get('meta').get('next_token')\n",
    "    #turn json to string and write to json file\n",
    "    json_response.pop('meta') \n",
    "    with open(os.getcwd()+'\\\\json_responses\\\\'+'KelloggStrike.json', 'w') as f:\n",
    "         json.dump(json_response, f, sort_keys = True, indent=4)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb1d18da-4894-41c5-8544-327459308aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonToCsv(jsonData, case):\n",
    "    if case=='tweet':\n",
    "        data_file = open(os.getcwd()+'\\\\CSV\\\\'+'Tweet.csv', 'w',encoding='utf-8',newline='')\n",
    "        csv_writer = csv.writer(data_file)\n",
    "        \n",
    "        #referenced tweets\n",
    "        data_file_RT = open(os.getcwd()+'\\\\CSV\\\\'+'Referenced_Tweet.csv', 'w',encoding='utf-8',newline='')\n",
    "        csv_writer_RT = csv.writer(data_file_RT)\n",
    "        count = 0\n",
    "        for var in jsonData:\n",
    "            if count == 0:\n",
    "                # Writing headers of CSV file\n",
    "                header = ['tweet_id','user_id','created_at','text','like_count','quote_count','reply_count','retweet_count','sentiment_score']\n",
    "                csv_writer.writerow(header)\n",
    "                \n",
    "                header = ['tweet_id','referenced_tweet_id','type']\n",
    "                csv_writer_RT.writerow(header)\n",
    "                count += 1\n",
    "            # Writing data of CSV file\n",
    "            \n",
    "            #calculate sentiment\n",
    "            score=sia.polarity_scores(var.get('text'))[\"compound\"]\n",
    "            \n",
    "            metrics=var.get('public_metrics')\n",
    "            row=[var.get('id'),var.get('author_id'),var.get('created_at'),\"'\"+var.get('text')+\"'\",metrics['like_count'],metrics['quote_count'],metrics['reply_count'],metrics['retweet_count'], score]\n",
    "            csv_writer.writerow(row)\n",
    "            \n",
    "            #rt\n",
    "            if 'referenced_tweets' in var:\n",
    "                rts=var.get('referenced_tweets')\n",
    "                for rt in rts:\n",
    "                    row=[var.get('id'),rt['id'], rt['type']]\n",
    "                    csv_writer_RT.writerow(row)\n",
    "        data_file.close()\n",
    "        data_file_RT.close()\n",
    "    if case=='user':\n",
    "        data_file = open(os.getcwd()+'\\\\CSV\\\\'+'User.csv', 'w',encoding='utf-8', newline='')\n",
    "        csv_writer = csv.writer(data_file)\n",
    "        count = 0\n",
    "        for var in jsonData:\n",
    "            if count == 0:\n",
    "                # Writing headers of CSV file\n",
    "                header = ['user_id','followers','following','tweet_count','listed_count','verified']\n",
    "                csv_writer.writerow(header)\n",
    "                count += 1\n",
    "            # Writing data of CSV file\n",
    "            metrics=var.get('public_metrics')\n",
    "            row=[var.get('id'),metrics['followers_count'],metrics['following_count'],metrics['tweet_count'],metrics['listed_count'], var.get('verified') ]\n",
    "            csv_writer.writerow(row)\n",
    "        data_file.close()\n",
    "    if case =='entities':\n",
    "        data_file_HT = open(os.getcwd()+'\\\\CSV\\\\'+'Hashtag_Tweet.csv', 'w',encoding='utf-8', newline='')\n",
    "        csv_writer_HT = csv.writer(data_file_HT)\n",
    "        \n",
    "        data_file_UT = open(os.getcwd()+'\\\\CSV\\\\'+'URL_Tweet.csv', 'w',encoding='utf-8', newline='')\n",
    "        csv_writer_UT = csv.writer(data_file_UT)\n",
    "            \n",
    "        data_file_MT = open(os.getcwd()+'\\\\CSV\\\\'+'Mention_Tweet.csv', 'w',encoding='utf-8', newline='')\n",
    "        csv_writer_MT = csv.writer(data_file_MT)\n",
    "            \n",
    "        count = 0\n",
    "        for var in jsonData:\n",
    "            if count == 0:\n",
    "                    # Writing headers of CSV file\n",
    "                header = ['tweet_id','hashtag']\n",
    "                csv_writer_HT.writerow(header)\n",
    "                    \n",
    "                header = ['tweet_id','urls']\n",
    "                csv_writer_UT.writerow(header)\n",
    "                    \n",
    "                header = ['tweet_id','mentions']\n",
    "                csv_writer_MT.writerow(header)\n",
    "                count += 1\n",
    "                    \n",
    "                # Writing data of CSV file\n",
    "            if 'entities' in var:\n",
    "                entities=var.get('entities')\n",
    "                if 'hashtags' in entities:\n",
    "                    for hashtag in entities['hashtags']:\n",
    "                        row=[var.get('id'),hashtag.get('tag')]\n",
    "                        csv_writer_HT.writerow(row)\n",
    "                if 'urls' in entities:\n",
    "                    for url in entities['urls']:\n",
    "                        row=[var.get('id'),url.get('expanded_url')]\n",
    "                        csv_writer_UT.writerow(row)\n",
    "                if 'mentions' in entities:\n",
    "                    for mention in entities['mentions']:\n",
    "                        row=[var.get('id'),mention.get('id')]\n",
    "                        csv_writer_MT.writerow(row)\n",
    "        data_file_HT.close()\n",
    "        data_file_UT.close()\n",
    "        data_file_MT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fc39005-39fd-4976-ba80-40ec81590027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write csv files\n",
    "with open(os.getcwd()+'\\\\json_responses\\\\'+'KelloggStrike.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "jsonData=data['data']\n",
    "jsonUser=data['includes']['users']\n",
    "#make tweets\n",
    "jsonToCsv(jsonData, 'tweet')\n",
    "jsonToCsv(jsonData, 'entities')\n",
    "jsonToCsv(jsonUser, 'user')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f232339-fffd-4fc9-84c2-5e62faa3a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonToCsv(jsonData, 'tweet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
